# Zoomprop ETL Template

A lightweight ETL template designed for rapid MVP development with clean data processing and focused unit testing.

## ğŸš€ Quick Start

```bash
# Clone the template
git clone <repository-url> my-etl-project
cd my-etl-project

# Install dependencies
pipenv install --dev

# Configure environment
cp .env.template .env
# Edit .env with your database credentials

# Run tests
pipenv run pytest

# Ready to build your ETL!
```

## ğŸ“ Project Structure

```raw
zoomprop-etl-template/
â”œâ”€â”€ README.md
â”œâ”€â”€ Pipfile                      # Python dependencies
â”œâ”€â”€ .env.template                # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml      # Code quality hooks
â”œâ”€â”€ LICENSE
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ etl_pipeline.py         # Main ETL orchestration
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ data_cleaner.py     # Data cleaning utilities
â”‚       â””â”€â”€ db_manager.py       # Database management
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_data_cleaner.py    # Data cleaning tests
    â”œâ”€â”€ test_db_manager.py      # Database operation tests
    â””â”€â”€ test_etl_pipeline.py    # Pipeline workflow tests
```

## âœ¨ Key Features

- **Smart Data Cleaning**: Handles 20+ null patterns (`""`, `"null"`, `"NaN"`, `"N/A"`, etc.)
- **Multi-Database Support**: PostgreSQL, MySQL, SQLite, SQL Server
- **Environment Configuration**: Multiple database connections via `.env`
- **Comprehensive Testing**: 100+ unit tests for reliability
- **Code Quality**: Pre-commit hooks with ruff formatting

## ğŸ¯ Usage Example

```python
from src.etl_pipeline import ETLPipeline

# Initialize pipeline
etl = ETLPipeline()

# Configure source and target
source_config = {
    'type': 'csv',
    'path': 'data/sales_data.csv'
}

target_config = {
    'table_name': 'cleaned_sales',
    'if_exists': 'replace'
}

# Add custom transformation
def add_metadata(df):
    import pandas as pd
    df['processed_at'] = pd.Timestamp.now()
    return df

# Run pipeline
etl.run_pipeline(
    source_config=source_config,
    target_config=target_config,
    custom_transformations=[add_metadata]
)
```

## âš™ï¸ Configuration

Create `.env` from `.env.template`:

```bash
# Database Configuration
DB_TYPE=postgresql
DB_HOST=localhost
DB_PORT=5432
DB_NAME=your_database
DB_USER=your_username
DB_PASSWORD=your_password

# Performance Settings
DB_CHUNK_SIZE=1000
LOG_LEVEL=INFO
```

## ğŸ§ª Testing

```bash
# Run all tests
pipenv run pytest

# With coverage
pipenv run pytest --cov=src

# Specific test file
pipenv run pytest tests/test_data_cleaner.py
```

## ğŸ”§ Extending

### Custom Transformations

```python
def your_business_logic(df):
    # Your transformation here
    df['new_column'] = df['old_column'] * 2
    return df

etl.run_pipeline(source_config, target_config, [your_business_logic])
```

### New Data Sources

Extend `etl_pipeline.py`:

```python
def extract_from_api(self, endpoint):
    import requests
    response = requests.get(endpoint)
    return pd.DataFrame(response.json())
```

Built with â¤ï¸ by the Zoomprop Engineering Team.
